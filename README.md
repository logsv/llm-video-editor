# LLM Video Editor

An intelligent, prompt-driven video router/editor that processes video content and outputs platform-ready cuts for YouTube (16:9) and Instagram Reels (9:16). **Now fully functional with local LLM support using Ollama!** üéâ

## üé¨ Features

- **Intelligent Analysis**: Automatic speech recognition (ASR) with word-level timestamps using Whisper
- **Scene Detection**: Smart boundary detection for optimal cuts using PySceneDetect
- **Local LLM Planning**: AI-driven Edit Decision List (EDL) generation using Ollama (no API keys required!)
- **Platform Optimization**: Pre-configured settings for YouTube (16:9), Instagram Reels (9:16), TikTok
- **Professional Video Rendering**: GPU-accelerated video processing with FFmpeg
- **Complete Offline Processing**: All processing runs locally - no external dependencies
- **Smart Reframing**: Automatic aspect ratio conversion (16:9 to 9:16) with intelligent cropping
- **Generative B-roll**: AI-generated video content using Open-Sora v2 (text‚Üívideo) and Stable Video Diffusion 1.1 (image‚Üívideo)
- **Conversational CLI**: Natural-language interface for interactive video editing with prompt clarification

## üöÄ Quick Start

### Installation

1. **Install Ollama** (for local LLM support)
   ```bash
   # macOS
   brew install ollama
   
   # Start Ollama service
   ollama serve
   
   # Pull a model (in another terminal)
   ollama pull llama3.1
   ```

2. **Create Python Environment**
   ```bash
   # Using conda (recommended)
   conda env create -f environment.yml
   conda activate llm-video-editor
   
   # Or using venv
   python -m venv venv
   source venv/bin/activate  # Linux/macOS
   # or venv\Scripts\activate  # Windows
   ```

3. **Install FFmpeg**
   ```bash
   # macOS
   brew install ffmpeg
   
   # Ubuntu
   sudo apt update && sudo apt install ffmpeg
   
   # Windows: Download from https://ffmpeg.org/
   ```

4. **Install Package**
   ```bash
   pip install -e .
   ```

### Basic Usage

**Using the Python API (Recommended):**
```python
from llm_video_editor.core.ollama_planner import OllamaVideoPlanner
from llm_video_editor.core.media_probe import MediaProbe
from llm_video_editor.core.asr import ASRProcessor
from llm_video_editor.core.scene_detection import SceneDetector
from llm_video_editor.core.renderer import VideoRenderer

# Initialize components
planner = OllamaVideoPlanner(model_name="llama3.1")
renderer = VideoRenderer(use_gpu=True)
probe = MediaProbe()

# Process video
media_info = probe.probe_file("input.mp4")
edl = planner.generate_edl(
    prompt="Create a 30s Instagram Reel highlighting the best moments",
    media_info=media_info,
    target_platform="reels"
)

# Render final video
results = renderer.render_edl(edl, "input.mp4", "output/")
print(f"Final video: {results['final_video']}")
```

**Conversational CLI:**
```bash
llm-video-router chat
# Commands: add <path>, add-image <path>, edit, t2v, i2v, merge, list, quit
```

**Command Line (Coming Soon):**
```bash
# Create a 60s Instagram Reel
llm-video-router -i video.mp4 -p "60s reel: top 3 takeaways, captions" -t reels

# Process multiple videos for YouTube  
llm-video-router -i ./videos -p "10min compilation: best moments" -t youtube
```

## üìã Requirements

### System Dependencies
- **Ollama** (for local LLM inference)
- **FFmpeg** (with VideoToolbox/NVENC support for GPU acceleration)
- **Python 3.11+**

### Python Dependencies (Automatically Installed)
- **LangGraph** for workflow orchestration
- **faster-whisper** for speech recognition with local Whisper models
- **PySceneDetect** for intelligent scene boundary detection  
- **OpenTimelineIO** for professional video interchange
- **langchain-community** for Ollama LLM integration
- **requests** for API communication

### Tested Configurations
- ‚úÖ **macOS**: Apple Silicon with VideoToolbox GPU acceleration
- ‚úÖ **Linux**: NVIDIA GPUs with NVENC acceleration
- ‚ö†Ô∏è **Windows**: CPU-only (GPU acceleration coming soon)

## üèóÔ∏è Architecture

```
Input Video ‚Üí Media Probe ‚Üí ASR + Scene Detection ‚Üí Ollama LLM Planning ‚Üí Video Rendering ‚Üí Platform-Ready Output
```

### Workflow Steps

1. **Media Probe**: Extract video metadata, resolution, duration, and codec information
2. **ASR Processing**: Transcribe audio with word-level timestamps using local Whisper models
3. **Scene Detection**: Identify shot boundaries and content changes using PySceneDetect
4. **LLM Planning**: Generate Edit Decision List (EDL) using local Ollama models based on user prompt
5. **Video Rendering**: Apply cuts, reframing, and effects with GPU-accelerated FFmpeg
6. **Output Generation**: Create platform-optimized videos with subtitles and metadata

### Key Improvements ‚ú®
- **100% Local Processing**: No API keys or internet required after initial setup
- **GPU Acceleration**: Hardware-accelerated video encoding (VideoToolbox/NVENC)
- **Smart Reframing**: Intelligent 16:9 to 9:16 conversion with content-aware cropping
- **Production Ready**: Generates actual video files, not just planning documents

## üéØ Platform Presets

| Platform | Resolution | Aspect Ratio | Max Duration | Typical Duration |
|----------|-----------|--------------|--------------|------------------|
| YouTube  | 1920x1080 | 16:9        | 1 hour       | 10 minutes       |
| Reels    | 1080x1920 | 9:16        | 90s          | 30s              |
| TikTok   | 1080x1920 | 9:16        | 3 minutes    | 60s              |

## üé® Generative B-roll

### Text‚ÜíVideo (Open-Sora v2)
Set `OPEN_SORA_CMD` to your local infer script, or `OPEN_SORA_HOST` to a running server.

### Image‚ÜíVideo (SVD 1.1)
Install `diffusers`, `torch`, `accelerate`, accept the model license on Hugging Face.

```bash
# Example usage in conversational CLI
llm-video-router chat
> t2v
Prompt for b-roll (text‚Üívideo): foggy city skyline timelapse, cinematic
Seconds [4]: 6
Resolution (e.g., 576x1024) [576x1024]: 
```

### Conversational CLI

```bash
llm-video-router chat
```

Available commands:
- `add <video_path>` - Add video asset to workspace
- `add-image <image_path>` - Add image asset for I2V generation
- `edit` - Edit video with natural language prompts
- `t2v` - Generate text-to-video B-roll clips
- `i2v` - Generate image-to-video B-roll clips  
- `merge` - Combine multiple video assets
- `list` - Show all assets in workspace
- `quit` - Exit the interface

## üîß CLI Commands

### Main Processing
```bash
llm-video-router [OPTIONS]

Options:
  -i, --input PATH       Input video file or directory [required]
  -p, --prompt TEXT      Editing prompt [required]
  -t, --target CHOICE    Target platform (youtube|reels|tiktok)
  -o, --output PATH      Output directory
  --asr-model CHOICE     Whisper model size
  --scene-threshold FLOAT Scene detection threshold
  --language TEXT        Language code for ASR
  --dry-run              Preview without processing
  --config PATH          Configuration file
```

### Utility Commands
```bash
# Analyze video file
llm-video-router info video.mp4

# Detect scenes
llm-video-router scenes video.mp4 --thumbnails

# Transcribe audio
llm-video-router transcribe video.mp4 --format srt

# List platforms
llm-video-router platforms
```

## üêç Programmatic Usage

```python
from llm_video_editor.core.workflow import create_workflow

# Create workflow
workflow = create_workflow(
    asr_model="large-v3",
    scene_threshold=27.0,
    planner_model="gpt-4"
)

# Process video
result = workflow.run({
    "path": "video.mp4",
    "prompt": "Create a 30s highlights reel with captions",
    "target": "reels",
    "output_dir": "output"
})

print(f"Status: {result['status']}")
print(f"Generated {result['edl_summary']['clips_count']} clips")
```

## ‚öôÔ∏è Configuration

Create a `config.json` file:

```json
{
  "asr_model": "large-v3",
  "scene_threshold": 27.0,
  "planner_model": "gpt-4",
  "language": "en",
  "gpu_acceleration": true,
  "platforms": {
    "reels": {
      "max_duration": 60,
      "quality": "medium"
    }
  }
}
```

## üìÅ Output Structure

```
output/
‚îú‚îÄ‚îÄ video_name/
‚îÇ   ‚îú‚îÄ‚îÄ edit_decision_list.json    # Main EDL file
‚îÇ   ‚îú‚îÄ‚îÄ captions.srt              # Subtitle file  
‚îÇ   ‚îú‚îÄ‚îÄ scenes.json               # Scene boundaries
‚îÇ   ‚îú‚îÄ‚îÄ validation_report.json    # Quality checks
‚îÇ   ‚îî‚îÄ‚îÄ thumbnails/               # Scene thumbnails
‚îî‚îÄ‚îÄ processing_results.json       # Batch results
```

## üé® Example Prompts

- `"60s reel: top 3 takeaways, captions, upbeat"`
- `"5min YouTube highlights: best moments, lower thirds"`
- `"30s product demo: step-by-step, call-to-action"`
- `"2min tutorial summary: key points, engaging intro"`
- `"45s testimonial compilation: emotional moments"`

## üîç Quality Assurance

The system performs automatic validation:

- ‚úÖ Duration constraints per platform
- ‚úÖ Aspect ratio compatibility  
- ‚úÖ Scene boundary alignment
- ‚úÖ Speech coverage analysis
- ‚úÖ Audio loudness standards (EBU-R128)

## üß™ Testing

### Quick Test
```python
# Test the complete workflow
python test_ollama_workflow.py
```

Expected output:
- ‚úÖ Media probing: Video metadata extraction
- ‚úÖ Scene detection: Boundary identification
- ‚úÖ ASR processing: Audio transcription
- ‚úÖ Ollama LLM: EDL generation
- ‚úÖ Video rendering: Platform-ready output

### Component Tests
```python
# Test individual components
python test_full_rendering.py    # Video rendering pipeline
python test_concat_fix.py       # Video concatenation
```

## üõ†Ô∏è Development

### Setup Development Environment
```bash
git clone <repository>
cd llm-video-editor
conda env create -f environment.yml
conda activate llm-video-editor
pip install -e .

# Install Ollama and pull model
ollama serve &
ollama pull llama3.1
```

### Run Tests
```bash
python -m pytest tests/         # Unit tests
python test_ollama_workflow.py  # End-to-end test
```

### Code Quality
```bash
black llm_video_editor/
flake8 llm_video_editor/
mypy llm_video_editor/
```

## üìö Advanced Features

### Custom LLM Models
```python
from langchain_openai import ChatOpenAI
from llm_video_editor.core.planner import VideoPlanner

# Use custom model
custom_llm = ChatOpenAI(model_name="gpt-3.5-turbo")
planner = VideoPlanner(llm=custom_llm)
```

### GPU Acceleration
```bash
# Enable NVENC for faster encoding
llm-video-router -i video.mp4 -p "quick edit" --gpu-accel
```

### Batch Processing
```bash
# Process entire directory
llm-video-router -i ./videos/ -p "social media cuts" -t reels
```

## üêõ Troubleshooting

### Common Issues

1. **FFmpeg not found**
   ```bash
   # Install via conda
   conda install ffmpeg
   
   # Or system package manager
   apt-get install ffmpeg  # Ubuntu
   brew install ffmpeg     # macOS
   ```

2. **CUDA/GPU issues**
   ```bash
   # Check NVENC support
   ffmpeg -encoders | grep nvenc
   
   # Fallback to CPU
   llm-video-router --no-gpu-accel
   ```

3. **Memory issues with large models**
   ```bash
   # Use smaller Whisper model
   llm-video-router --asr-model medium
   ```

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìû Support

- üìñ Documentation: [docs/](docs/)
- üêõ Bug Reports: [GitHub Issues](../../issues)
- üí° Feature Requests: [GitHub Issues](../../issues)
- üí¨ Discussions: [GitHub Discussions](../../discussions)

---

**Built with ‚ù§Ô∏è using LangGraph, Whisper, and FFmpeg**