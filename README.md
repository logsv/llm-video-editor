# LLM Video Editor

An intelligent, prompt-driven video router/editor that processes video content and outputs platform-ready cuts for YouTube (16:9) and Instagram Reels (9:16).

## ğŸ¬ Features

- **Intelligent Analysis**: Automatic speech recognition (ASR) with word-level timestamps
- **Scene Detection**: Smart boundary detection for optimal cuts
- **LLM Planning**: AI-driven Edit Decision List (EDL) generation from natural language prompts
- **Platform Optimization**: Pre-configured settings for YouTube, Instagram Reels, TikTok
- **Professional Output**: Exports EDL, SRT captions, and validation reports
- **GPU Acceleration**: NVENC hardware encoding support for faster rendering

## ğŸš€ Quick Start

### Installation

1. **Create Conda Environment**
   ```bash
   conda env create -f environment.yml
   conda activate llm-video-editor
   ```

2. **Install Package**
   ```bash
   pip install -e .
   ```

### Basic Usage

```bash
# Create a 60s Instagram Reel
llm-video-router -i video.mp4 -p "60s reel: top 3 takeaways, captions" -t reels

# Process multiple videos for YouTube
llm-video-router -i ./videos -p "10min compilation: best moments" -t youtube

# Custom configuration
llm-video-router -i video.mp4 -p "Resumen en 30s" -t reels --language es --config config.json
```

## ğŸ“‹ Requirements

### System Dependencies
- **FFmpeg** (with NVENC support for GPU acceleration)
- **Python 3.11+**
- **CUDA** (optional, for GPU acceleration)

### Python Dependencies
- LangGraph for workflow orchestration
- faster-whisper for speech recognition
- PySceneDetect for scene boundary detection
- OpenTimelineIO for professional interchange
- OpenAI/Transformers for LLM planning

## ğŸ—ï¸ Architecture

```
Input Video â†’ Probe â†’ ASR + Scene Detection â†’ LLM Planning â†’ Validation â†’ Output
```

### Workflow Steps

1. **Probe**: Extract media metadata and basic analysis
2. **ASR**: Transcribe with word-level timestamps using Whisper
3. **Scene Detection**: Identify shot boundaries using content analysis
4. **Planning**: Generate Edit Decision List using LLM based on user prompt
5. **Validation**: Quality checks and constraint validation
6. **Export**: EDL, captions, thumbnails, and validation reports

## ğŸ¯ Platform Presets

| Platform | Resolution | Aspect Ratio | Max Duration | Typical Duration |
|----------|-----------|--------------|--------------|------------------|
| YouTube  | 1920x1080 | 16:9        | 1 hour       | 10 minutes       |
| Reels    | 1080x1920 | 9:16        | 90s          | 30s              |
| TikTok   | 1080x1920 | 9:16        | 3 minutes    | 60s              |

## ğŸ”§ CLI Commands

### Main Processing
```bash
llm-video-router [OPTIONS]

Options:
  -i, --input PATH       Input video file or directory [required]
  -p, --prompt TEXT      Editing prompt [required]
  -t, --target CHOICE    Target platform (youtube|reels|tiktok)
  -o, --output PATH      Output directory
  --asr-model CHOICE     Whisper model size
  --scene-threshold FLOAT Scene detection threshold
  --language TEXT        Language code for ASR
  --dry-run              Preview without processing
  --config PATH          Configuration file
```

### Utility Commands
```bash
# Analyze video file
llm-video-router info video.mp4

# Detect scenes
llm-video-router scenes video.mp4 --thumbnails

# Transcribe audio
llm-video-router transcribe video.mp4 --format srt

# List platforms
llm-video-router platforms
```

## ğŸ Programmatic Usage

```python
from llm_video_editor.core.workflow import create_workflow

# Create workflow
workflow = create_workflow(
    asr_model="large-v3",
    scene_threshold=27.0,
    planner_model="gpt-4"
)

# Process video
result = workflow.run({
    "path": "video.mp4",
    "prompt": "Create a 30s highlights reel with captions",
    "target": "reels",
    "output_dir": "output"
})

print(f"Status: {result['status']}")
print(f"Generated {result['edl_summary']['clips_count']} clips")
```

## âš™ï¸ Configuration

Create a `config.json` file:

```json
{
  "asr_model": "large-v3",
  "scene_threshold": 27.0,
  "planner_model": "gpt-4",
  "language": "en",
  "gpu_acceleration": true,
  "platforms": {
    "reels": {
      "max_duration": 60,
      "quality": "medium"
    }
  }
}
```

## ğŸ“ Output Structure

```
output/
â”œâ”€â”€ video_name/
â”‚   â”œâ”€â”€ edit_decision_list.json    # Main EDL file
â”‚   â”œâ”€â”€ captions.srt              # Subtitle file  
â”‚   â”œâ”€â”€ scenes.json               # Scene boundaries
â”‚   â”œâ”€â”€ validation_report.json    # Quality checks
â”‚   â””â”€â”€ thumbnails/               # Scene thumbnails
â””â”€â”€ processing_results.json       # Batch results
```

## ğŸ¨ Example Prompts

- `"60s reel: top 3 takeaways, captions, upbeat"`
- `"5min YouTube highlights: best moments, lower thirds"`
- `"30s product demo: step-by-step, call-to-action"`
- `"2min tutorial summary: key points, engaging intro"`
- `"45s testimonial compilation: emotional moments"`

## ğŸ” Quality Assurance

The system performs automatic validation:

- âœ… Duration constraints per platform
- âœ… Aspect ratio compatibility  
- âœ… Scene boundary alignment
- âœ… Speech coverage analysis
- âœ… Audio loudness standards (EBU-R128)

## ğŸ› ï¸ Development

### Setup Development Environment
```bash
git clone <repository>
cd llm-video-editor
conda env create -f environment.yml
conda activate llm-video-editor
pip install -e .
```

### Run Tests
```bash
python -m pytest tests/
```

### Code Quality
```bash
black llm_video_editor/
flake8 llm_video_editor/
mypy llm_video_editor/
```

## ğŸ“š Advanced Features

### Custom LLM Models
```python
from langchain_openai import ChatOpenAI
from llm_video_editor.core.planner import VideoPlanner

# Use custom model
custom_llm = ChatOpenAI(model_name="gpt-3.5-turbo")
planner = VideoPlanner(llm=custom_llm)
```

### GPU Acceleration
```bash
# Enable NVENC for faster encoding
llm-video-router -i video.mp4 -p "quick edit" --gpu-accel
```

### Batch Processing
```bash
# Process entire directory
llm-video-router -i ./videos/ -p "social media cuts" -t reels
```

## ğŸ› Troubleshooting

### Common Issues

1. **FFmpeg not found**
   ```bash
   # Install via conda
   conda install ffmpeg
   
   # Or system package manager
   apt-get install ffmpeg  # Ubuntu
   brew install ffmpeg     # macOS
   ```

2. **CUDA/GPU issues**
   ```bash
   # Check NVENC support
   ffmpeg -encoders | grep nvenc
   
   # Fallback to CPU
   llm-video-router --no-gpu-accel
   ```

3. **Memory issues with large models**
   ```bash
   # Use smaller Whisper model
   llm-video-router --asr-model medium
   ```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ Support

- ğŸ“– Documentation: [docs/](docs/)
- ğŸ› Bug Reports: [GitHub Issues](../../issues)
- ğŸ’¡ Feature Requests: [GitHub Issues](../../issues)
- ğŸ’¬ Discussions: [GitHub Discussions](../../discussions)

---

**Built with â¤ï¸ using LangGraph, Whisper, and FFmpeg**